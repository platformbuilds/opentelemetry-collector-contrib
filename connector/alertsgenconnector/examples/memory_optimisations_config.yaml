# Memory-Optimized AlertsGen Connector Configuration Examples
# Choose the configuration that matches your deployment scale

# ==============================================================================
# SMALL DEPLOYMENT (< 10K spans/sec)
# ==============================================================================
connectors:
  alertsgen/small:
    instance_id: "collector-small-${HOSTNAME}"
    window: 5s
    step: 5s
    
    # Memory configuration for small deployments
    memory:
      # Use 5% of system memory
      max_memory_percent: 0.05
      
      # Fixed buffer sizes (good for predictable workloads)
      max_trace_entries: 25000    # ~25MB
      max_log_entries: 50000      # ~25MB  
      max_metric_entries: 100000  # ~25MB
      
      # Conservative scaling
      enable_adaptive_scaling: true
      scale_up_threshold: 0.9     # Scale up at 90%
      scale_down_threshold: 0.3   # Scale down below 30%
      scale_check_interval: 60s
      max_scale_factor: 3.0       # Allow 3x scaling
      
      # Memory pressure handling
      enable_memory_pressure_handling: true
      memory_pressure_threshold: 0.8
      sampling_rate_under_pressure: 0.5  # Drop 50% under pressure
      
      # Use simple slice buffers
      use_ring_buffers: false

# ==============================================================================
# MEDIUM DEPLOYMENT (10K-100K spans/sec)
# ==============================================================================
connectors:
  alertsgen/medium:
    instance_id: "collector-medium-${HOSTNAME}"
    window: 10s    # Slightly larger window
    step: 15s      # Less frequent evaluation
    
    memory:
      # Use 10% of system memory
      max_memory_percent: 0.10
      
      # Auto-calculate buffer sizes based on memory
      max_trace_entries: 0   # Auto-calculate
      max_log_entries: 0     # Auto-calculate
      max_metric_entries: 0  # Auto-calculate
      
      # Aggressive adaptive scaling
      enable_adaptive_scaling: true
      scale_up_threshold: 0.8
      scale_down_threshold: 0.4
      scale_check_interval: 30s
      max_scale_factor: 5.0   # Allow 5x scaling
      
      # Enhanced memory pressure handling
      enable_memory_pressure_handling: true
      memory_pressure_threshold: 0.85
      sampling_rate_under_pressure: 0.2  # Drop 80% under pressure
      
      # Use ring buffers for better memory efficiency
      use_ring_buffers: true
      ring_buffer_overwrite: true  # Overwrite old data

# ==============================================================================
# LARGE DEPLOYMENT (100K-1M spans/sec)
# ==============================================================================
connectors:
  alertsgen/large:
    instance_id: "collector-large-${HOSTNAME}"
    window: 30s    # Larger window for aggregation
    step: 30s      # Match window size
    
    memory:
      # Use 20% of system memory
      max_memory_percent: 0.20
      
      # Large fixed buffers with auto-scaling
      max_trace_entries: 500000   # Start with 500K
      max_log_entries: 1000000    # Start with 1M
      max_metric_entries: 2000000 # Start with 2M
      
      # Very aggressive scaling for high throughput
      enable_adaptive_scaling: true
      scale_up_threshold: 0.7     # Scale up early
      scale_down_threshold: 0.3
      scale_check_interval: 15s   # Check frequently
      max_scale_factor: 10.0      # Allow 10x scaling
      
      # Strict memory pressure handling
      enable_memory_pressure_handling: true
      memory_pressure_threshold: 0.9  # Higher threshold
      sampling_rate_under_pressure: 0.1  # Drop 90% under pressure
      
      # Ring buffers with overwrite for guaranteed memory bounds
      use_ring_buffers: true
      ring_buffer_overwrite: true

# ==============================================================================
# ENTERPRISE DEPLOYMENT (1M+ spans/sec)
# ==============================================================================
connectors:
  alertsgen/enterprise:
    instance_id: "collector-enterprise-${HOSTNAME}"
    window: 60s    # Large window for high-volume aggregation
    step: 60s
    
    memory:
      # Use explicit memory limit
      max_memory_bytes: 8589934592  # 8GB explicit limit
      
      # Very large buffers
      max_trace_entries: 2000000
      max_log_entries: 4000000
      max_metric_entries: 8000000
      
      # Continuous adaptive scaling
      enable_adaptive_scaling: true
      scale_up_threshold: 0.6     # Scale up early and often
      scale_down_threshold: 0.2
      scale_check_interval: 10s   # Very frequent checks
      max_scale_factor: 20.0      # Allow massive scaling
      
      # Advanced memory pressure handling
      enable_memory_pressure_handling: true
      memory_pressure_threshold: 0.95  # Very high threshold
      sampling_rate_under_pressure: 0.05  # Drop 95% under pressure
      
      # Ring buffers mandatory for memory safety
      use_ring_buffers: true
      ring_buffer_overwrite: true

# ==============================================================================
# DEVELOPMENT/TESTING (minimal memory usage)
# ==============================================================================
connectors:
  alertsgen/dev:
    instance_id: "collector-dev"
    window: 5s
    step: 5s
    
    memory:
      # Very conservative memory usage
      max_memory_percent: 0.02  # Only 2% of system memory
      
      # Small fixed buffers
      max_trace_entries: 1000
      max_log_entries: 1000
      max_metric_entries: 1000
      
      # Disable adaptive scaling for predictability
      enable_adaptive_scaling: false
      
      # Aggressive memory pressure handling
      enable_memory_pressure_handling: true
      memory_pressure_threshold: 0.7
      sampling_rate_under_pressure: 0.8  # Keep 80% under pressure
      
      # Simple slice buffers
      use_ring_buffers: false

# ==============================================================================
# MONITORING CONFIGURATION
# ==============================================================================
# Add these to service.telemetry.metrics for memory monitoring
service:
  telemetry:
    metrics:
      level: detailed
      address: 0.0.0.0:8888
    logs:
      level: info
      
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [alertsgen/large]  # Choose appropriate config
      
    logs:
      receivers: [otlp]
      processors: [batch]
      exporters: [alertsgen/large]
      
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [alertsgen/large]

# ==============================================================================
# GRAFANA DASHBOARD QUERIES
# ==============================================================================
# Use these Prometheus queries to monitor memory usage:

# Memory usage percentage
# otel_alert_memory_usage_percent

# Buffer utilization by signal type
# otel_alert_buffer_size{metric_type="current"} / otel_alert_buffer_size{metric_type="capacity"}

# Data drop rate
# rate(otel_alert_data_dropped_total[5m])

# Scale events
# rate(otel_alert_scale_events_total[5m])

# Memory pressure events
# rate(otel_alert_scale_events_total{event_type="memory_pressure"}[5m])

# ==============================================================================
# TROUBLESHOOTING CONFIGURATION
# ==============================================================================
# If experiencing memory issues, try this debug configuration:

connectors:
  alertsgen/debug:
    instance_id: "collector-debug"
    window: 5s
    
    memory:
      # Very small limits to force scaling events
      max_trace_entries: 100
      max_log_entries: 100
      max_metric_entries: 100
      
      # Frequent scaling for debugging
      enable_adaptive_scaling: true
      scale_up_threshold: 0.5
      scale_down_threshold: 0.2
      scale_check_interval: 5s
      max_scale_factor: 2.0
      
      # Immediate memory pressure
      enable_memory_pressure_handling: true
      memory_pressure_threshold: 0.6
      sampling_rate_under_pressure: 0.9
      
      use_ring_buffers: true
      ring_buffer_overwrite: true

# Set log level to debug to see detailed memory operations:
service:
  telemetry:
    logs:
      level: debug